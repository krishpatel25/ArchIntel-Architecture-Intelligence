{
  "timestamp": 1755048867.466617,
  "papers": [
    {
      "id": "2309.16509v1",
      "title": "SIMD Everywhere Optimization from ARM NEON to RISC-V Vector Extensions",
      "summary": "Many libraries, such as OpenCV, FFmpeg, XNNPACK, and Eigen, utilize Arm or\nx86 SIMD Intrinsics to optimize programs for performance. With the emergence of\nRISC-V Vector Extensions (RVV), there is a need to migrate these performance\nlegacy codes for RVV. Currently, the migration of NEON code to RVV code\nrequires manual rewriting, which is a time-consuming and error-prone process.\nIn this work, we use the open source tool, \"SIMD Everywhere\" (SIMDe), to\nautomate the migration. Our primary task is to enhance SIMDe to enable the\nconversion of ARM NEON Intrinsics types and functions to their corresponding\nRVV Intrinsics types and functions. For type conversion, we devise strategies\nto convert Neon Intrinsics types to RVV Intrinsics by considering the vector\nlength agnostic (vla) architectures. With function conversions, we analyze\ncommonly used conversion methods in SIMDe and develop customized conversions\nfor each function based on the results of RVV code generations. In our\nexperiments with Google XNNPACK library, our enhanced SIMDe achieves speedup\nranging from 1.51x to 5.13x compared to the original SIMDe, which does not\nutilize customized RVV implementations for the conversions.",
      "pdf_url": "https://arxiv.org/pdf/2309.16509v1.pdf",
      "source": "arxiv",
      "text": "Many libraries, such as OpenCV, FFmpeg, XNNPACK, and Eigen, utilize Arm or\nx86 SIMD Intrinsics to optimize programs for performance. With the emergence of\nRISC-V Vector Extensions (RVV), there is a need to migrate these performance\nlegacy codes for RVV. Currently, the migration of NEON code to RVV code\nrequires manual rewriting, which is a time-consuming and error-prone process.\nIn this work, we use the open source tool, \"SIMD Everywhere\" (SIMDe), to\nautomate the migration. Our primary task is to enhance SIMDe to enable the\nconversion of ARM NEON Intrinsics types and functions to their corresponding\nRVV Intrinsics types and functions. For type conversion, we devise strategies\nto convert Neon Intrinsics types to RVV Intrinsics by considering the vector\nlength agnostic (vla) architectures. With function conversions, we analyze\ncommonly used conversion methods in SIMDe and develop customized conversions\nfor each function based on the results of RVV code generations. In our\nexperiments with Google XNNPACK library, our enhanced SIMDe achieves speedup\nranging from 1.51x to 5.13x compared to the original SIMDe, which does not\nutilize customized RVV implementations for the conversions."
    },
    {
      "id": "2507.01457v1",
      "title": "Tensor Program Optimization for the RISC-V Vector Extension Using\n  Probabilistic Programs",
      "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions.",
      "pdf_url": "https://arxiv.org/pdf/2507.01457v1.pdf",
      "source": "arxiv",
      "text": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions."
    },
    {
      "id": "2304.10319v1",
      "title": "Test-driving RISC-V Vector hardware for HPC",
      "summary": "Whilst the RISC-V Vector extension (RVV) has been ratified, at the time of\nwriting both hardware implementations and open source software support are\nstill limited for vectorisation on RISC-V. This is important because\nvectorisation is crucial to obtaining good performance for High Performance\nComputing (HPC) workloads and, as of April 2023, the Allwinner D1 SoC,\ncontaining the XuanTie C906 processor, is the only mass-produced and\ncommercially available hardware supporting RVV. This paper surveys the current\nstate of RISC-V vectorisation as of 2023, reporting the landscape of both the\nhardware and software ecosystem. Driving our discussion from experiences in\nsetting up the Allwinner D1 as part of the EPCC RISC-V testbed, we report the\nresults of benchmarking the Allwinner D1 using the RAJA Performance Suite,\nwhich demonstrated reasonable vectorisation speedup using vendor-provided\ncompiler, as well as favourable performance compared to the StarFive VisionFive\nV2 with SiFive's U74 processor.",
      "pdf_url": "https://arxiv.org/pdf/2304.10319v1.pdf",
      "source": "arxiv",
      "text": "Whilst the RISC-V Vector extension (RVV) has been ratified, at the time of\nwriting both hardware implementations and open source software support are\nstill limited for vectorisation on RISC-V. This is important because\nvectorisation is crucial to obtaining good performance for High Performance\nComputing (HPC) workloads and, as of April 2023, the Allwinner D1 SoC,\ncontaining the XuanTie C906 processor, is the only mass-produced and\ncommercially available hardware supporting RVV. This paper surveys the current\nstate of RISC-V vectorisation as of 2023, reporting the landscape of both the\nhardware and software ecosystem. Driving our discussion from experiences in\nsetting up the Allwinner D1 as part of the EPCC RISC-V testbed, we report the\nresults of benchmarking the Allwinner D1 using the RAJA Performance Suite,\nwhich demonstrated reasonable vectorisation speedup using vendor-provided\ncompiler, as well as favourable performance compared to the StarFive VisionFive\nV2 with SiFive's U74 processor."
    },
    {
      "id": "2304.10324v1",
      "title": "Backporting RISC-V Vector assembly",
      "summary": "Leveraging vectorisation, the ability for a CPU to apply operations to\nmultiple elements of data concurrently, is critical for high performance\nworkloads. However, at the time of writing, commercially available physical\nRISC-V hardware that provides the RISC-V vector extension (RVV) only supports\nversion 0.7.1, which is incompatible with the latest ratified version 1.0. The\nchallenge is that upstream compiler toolchains, such as Clang, only target the\nratified v1.0 and do not support the older v0.7.1. Because v1.0 is not\ncompatible with v0.7.1, the only way to program vectorised code is to use a\nvendor-provided, older compiler. In this paper we introduce the rvv-rollback\ntool which translates assembly code generated by the compiler using vector\nextension v1.0 instructions to v0.7.1. We utilise this tool to compare\nvectorisation performance of the vendor-provided GNU 8.4 compiler (supports\nv0.7.1) against LLVM 15.0 (supports only v1.0), where we found that the LLVM\ncompiler is capable of auto-vectorising more computational kernels, and\ndelivers greater performance than GNU in most, but not all, cases. We also\ntested LLVM vectorisation with vector length agnostic and specific settings,\nand observed cases with significant difference in performance.",
      "pdf_url": "https://arxiv.org/pdf/2304.10324v1.pdf",
      "source": "arxiv",
      "text": "Leveraging vectorisation, the ability for a CPU to apply operations to\nmultiple elements of data concurrently, is critical for high performance\nworkloads. However, at the time of writing, commercially available physical\nRISC-V hardware that provides the RISC-V vector extension (RVV) only supports\nversion 0.7.1, which is incompatible with the latest ratified version 1.0. The\nchallenge is that upstream compiler toolchains, such as Clang, only target the\nratified v1.0 and do not support the older v0.7.1. Because v1.0 is not\ncompatible with v0.7.1, the only way to program vectorised code is to use a\nvendor-provided, older compiler. In this paper we introduce the rvv-rollback\ntool which translates assembly code generated by the compiler using vector\nextension v1.0 instructions to v0.7.1. We utilise this tool to compare\nvectorisation performance of the vendor-provided GNU 8.4 compiler (supports\nv0.7.1) against LLVM 15.0 (supports only v1.0), where we found that the LLVM\ncompiler is capable of auto-vectorising more computational kernels, and\ndelivers greater performance than GNU in most, but not all, cases. We also\ntested LLVM vectorisation with vector length agnostic and specific settings,\nand observed cases with significant difference in performance."
    },
    {
      "id": "2407.13326v1",
      "title": "RISC-V RVV efficiency for ANN algorithms",
      "summary": "Handling vast amounts of data is crucial in today's world. The growth of\nhigh-performance computing has created a need for parallelization, particularly\nin the area of machine learning algorithms such as ANN (Approximate Nearest\nNeighbors). To improve the speed of these algorithms, it is important to\noptimize them for specific processor architectures. RISC-V (Reduced Instruction\nSet Computer Five) is one of the modern processor architectures, which features\na vector instruction set called RVV (RISC-V Vector Extension). In machine\nlearning algorithms, vector extensions are widely utilized to improve the\nprocessing of voluminous data. This study examines the effectiveness of\napplying RVV to commonly used ANN algorithms. The algorithms were adapted for\nRISC-V and optimized using RVV after identifying the primary bottlenecks.\nAdditionally, we developed a theoretical model of a parameterized vector block\nand identified the best on average configuration that demonstrates the highest\ntheoretical performance of the studied ANN algorithms when the other CPU\nparameters are fixed.",
      "pdf_url": "https://arxiv.org/pdf/2407.13326v1.pdf",
      "source": "arxiv",
      "text": "Handling vast amounts of data is crucial in today's world. The growth of\nhigh-performance computing has created a need for parallelization, particularly\nin the area of machine learning algorithms such as ANN (Approximate Nearest\nNeighbors). To improve the speed of these algorithms, it is important to\noptimize them for specific processor architectures. RISC-V (Reduced Instruction\nSet Computer Five) is one of the modern processor architectures, which features\na vector instruction set called RVV (RISC-V Vector Extension). In machine\nlearning algorithms, vector extensions are widely utilized to improve the\nprocessing of voluminous data. This study examines the effectiveness of\napplying RVV to commonly used ANN algorithms. The algorithms were adapted for\nRISC-V and optimized using RVV after identifying the primary bottlenecks.\nAdditionally, we developed a theoretical model of a parameterized vector block\nand identified the best on average configuration that demonstrates the highest\ntheoretical performance of the studied ANN algorithms when the other CPU\nparameters are fixed."
    }
  ]
}